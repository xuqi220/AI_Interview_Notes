# 分布式训练技术

## Pytorch分布式基础

## 分布式计算工具
### DeepSpeed

* DeepSpeed基础

## 实践


## Reference


# 高效微调技术(PEFT)

## Additive PEFT

## Selective PEFT

## Reparameterization PEFT

* Lora
  
  对于预训练模型中参数矩阵 $W_0\in R^{d\times k}$，Lora 引入了两个矩阵 $W_{up}\in R^{d\times r}$ 和 $W_{down}\in R^{r\times k}$，其中 $r\ll min(k,d)$。

    $H_{out}=W_0H_{in}+\frac{\alpha}{r}W_{up}W{down}H_{in}$
    
    其中，$H_{out},H_{in}$ 分别表示输出和输入; $\alpha$ 表示缩放因子（scaling factor）。
  <img src="./asset/lora.png">

## Hybrid PEFT

## Reference
[Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey](https://openreview.net/forum?id=lIsCS8b6zj)
