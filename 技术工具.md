# 分布式训练技术

## Pytorch分布式基础

## 分布式计算工具
### DeepSpeed

* DeepSpeed基础

## 实践


## Reference


# 高效微调技术(PEFT)

## Additive PEFT

## Selective PEFT

## Reparameterization PEFT

### Lora
* 介绍
  
    <img src="./asset/lora.png" style="width:200px">
  
  对于预训练模型中参数矩阵 $W_0\in R^{d\times k}$，Lora 引入了两个矩阵 $W_{up}\in R^{d\times r}$ 和 $W_{down}\in R^{r\times k}$，其中 $r\ll min(k,d)$。计算过程如下：

    $$H_{out}=W_0H_{in}+\frac{\alpha}{r}\Delta W H_{in} = W_0H_{in}+ \frac{\alpha}{r}W_{up}W_{down}H_{in}$$
    
    其中， $H_{out},H_{in}$ 分别表示输出和输入; $\alpha$ 表示缩放因子（scaling factor）。训练过程中， $W_{down}$ is initialized using a random Gaussian distribution, while $W_{up}$ is initialized to zero, ensuring that $\Delta W$ initially holds a value of zero. $W_0$ 不做更新， $W_{up}, W_{down}$ 更新。
* 问题
  
  1.  $W_{down}$ $W_{up}$ 初始化问题？
   https://zhuanlan.zhihu.com/p/1915822821
* 实践
  
  [大模型微调](https://github.com/xuqi220/QLaw)
  

## Hybrid PEFT

## Reference
[Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey](https://openreview.net/forum?id=lIsCS8b6zj)
