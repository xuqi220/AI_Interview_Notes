## KV_Cache
推理过程中，LLM以auto-regressive的方式迭代生成新的token，每一轮LLM只生成一个Token，新生成的token将与之前的tokens拼接，并作为下一轮的输入。例如：LLM以 $[t_1,t_2,...,t_{i-1}]$ 作为输入，经过层层的Attention、FNN计算后, $t_{i-1}$ 的表示向量（融合了 $t_{0～i-1}$ 的信息）将用于预测第 $t_{i}$ 个token，然后将它们拼接 $[t_1,t_2,...,t_{i-1},t_{i}]$，作为下一轮的输入预测第 $t_{i+1}$ 个token。

每一轮迭代都会重新计算已生成的每个token的 $Q,K,V$ 表示向量，由于 $t_{i}$ 相关的 $Q,K,V$ 仅依赖于 $t_{0～i}$，新生成的 $t_{i+1}$ 并不参与计算，因此navie的方法里面有大量的重复计算。KV_Cache策略以空间换时间将已经计算好的 $K,V$ 缓存起来，避免重复计算。当LM以 $[t_1,t_2,...,t_{i-1}]$ 作为输入预测 $t_{i}$ 时，$[t_1,t_2,...,t_{i-1}]$ 的 $K,V$ 已经计算好了，下一轮的输入 $[t_1,t_2,...,t_{i-1},t_{i}]$ 中只有 $t_{i}$ 的 $Q,K,V$ 需要从新计算用于预测 $t_{i+1}$

* **不使用KV_cache有多少重复计算？**
  
  如果 `max_new_tokens=n` 那么第i个token被重新计算了 $(n-1-i)*L$ 次，其中 $L$ 表示LLM 的层数。

* **为什么只缓存K，V不缓存Q？**
  
  因为根据Attention计算公式，当预测 $t_{i}$ 时，依赖于 $q_{t_{i}}$ $[t_1,t_2,...,t_{i-1}]$ 的 $K,V$。

* **缓存的内存怎么计算？**
  
  $L\times b\times l\times 2\times n_{head}\times d_{head}$
  
  解释如下：

  * 每个token的 $K,V$ 缓存量(Byte)：$2\times n_{head}\times d_{head}$
  * 所有token的 $K,V$ 缓存量(Byte)：$b\times l\times 2\times n_{head}\times d_{head}$。 $b,l$分别表示batch_size和上下文长度。
  * 由于LLM的每一层都需要缓存，所以最后总的缓存量(Byte)为：$L\times b\times l\times 2\times n_{head}\times d_{head}$。$L$表示LLM的层数。



量化
模型压缩 https://mp.weixin.qq.com/s/3I4nZcCvc7DgZ5xQUTfAOQ
